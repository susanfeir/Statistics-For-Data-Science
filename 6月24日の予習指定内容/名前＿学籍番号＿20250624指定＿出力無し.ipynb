{"cells":[{"cell_type":"markdown","source":["# 単回帰分析の結果の手計算"],"metadata":{"id":"BTNqPpPIh118"}},{"cell_type":"markdown","source":["## (1) statsmodels formula APIの線形回帰分析用のOLS関数を利用"],"metadata":{"id":"B20TphgRiubf"}},{"cell_type":"code","source":["# 数値計算に使うライブラリ\n","import numpy as np\n","import pandas as pd\n","from scipy import stats\n","# 表示桁数の設定\n","pd.set_option('display.precision', 3)\n","np.set_printoptions(precision=3)\n","\n","# グラフを描画するライブラリ\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","sns.set()\n","\n","# 統計モデルを推定するライブラリ\n","import statsmodels.formula.api as smf\n","import statsmodels.api as sm"],"metadata":{"id":"insNGstveWMi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 架空のデータフレームを作成（temperatureとbeerの関係）\n","np.random.seed(42)  # 再現性のため\n","\n","# temperatureとbeerの8個のデータポイント\n","temperature = np.array([15.2, 18.5, 22.1, 25.3, 28.9, 31.2, 16.8, 19.4])\n","\n","beer = np.array([42.1, 45.8, 49.2, 53.7, 57.3, 58.9, 43.2, 46.1])\n","\n","# データフレーム作成\n","df = pd.DataFrame({\n","    'temperature': temperature,\n","    'beer': beer\n","})\n","\n","df.head()"],"metadata":{"id":"p7tyBkENiAtK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# statsmodels formula APIの線形回帰分析用のOLS関数を利用\n","model = smf.ols('beer ~ temperature', data=df)\n","results = model.fit()\n","\n","# 線形回帰分析結果をまとめる\n","print(results.summary())\n","\n","# # 個別の決定係数、回帰係数、切片をプリントアウト\n","# print(\"R-squared:\", results.rsquared)\n","# print(\"Coefficients:\\n\", results.params)"],"metadata":{"id":"xv3M8z3Rd78j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## (2) 上のoutput線形回帰分析結果を、今まで勉強した内容を利用して手計算"],"metadata":{"id":"8glvy4MRi4Rq"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from scipy import stats\n","import matplotlib.pyplot as plt\n","\n","# 架空のデータフレームを作成（temperatureとbeerの関係）\n","np.random.seed(42)  # 再現性のため\n","\n","# temperatureとbeerの8個のデータポイント\n","temperature = np.array([15.2, 18.5, 22.1, 25.3, 28.9, 31.2, 16.8, 19.4])\n","\n","beer = np.array([42.1, 45.8, 49.2, 53.7, 57.3, 58.9, 43.2, 46.1])\n","\n","# データフレーム作成\n","df = pd.DataFrame({\n","    'temperature': temperature,\n","    'beer': beer\n","})\n","\n","print(\"作成したデータフレーム:\")\n","print(df.head(10))\n","print(f\"\\nデータ数: {len(df)}\")"],"metadata":{"id":"-2SjzVhC4G7d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================\n","# 1. 基本統計量の計算\n","# ===============================\n","n = len(df)\n","x = df['temperature']\n","y = df['beer']\n","\n","x_mean = np.mean(x)\n","y_mean = np.mean(y)\n","\n","print(f\"\\n基本統計量:\")\n","print(f\"サンプル数 (n): {n}\")\n","print(f\"temperature平均: {x_mean:.4f}\")\n","print(f\"beer平均: {y_mean:.4f}\")"],"metadata":{"id":"KVQ4KhDsjUGZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================\n","# 2. 回帰係数の手計算\n","# ===============================\n","# β1 (slope) = Σ(xi - x̄)(yi - ȳ) / Σ(xi - x̄)²\n","numerator = np.sum((x - x_mean) * (y - y_mean))\n","denominator = np.sum((x - x_mean)**2)\n","beta1 = numerator / denominator\n","\n","# β0 (intercept) = ȳ - β1 * x̄\n","beta0 = y_mean - beta1 * x_mean\n","\n","print(f\"\\n回帰係数の手計算:\")\n","print(f\"β1 (slope): {beta1:.4f}\")\n","print(f\"β0 (intercept): {beta0:.4f}\")"],"metadata":{"id":"T2H_EHm-fzEk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================\n","# 3. 予測値と残差の計算\n","# ===============================\n","y_pred = beta0 + beta1 * x\n","residuals = y - y_pred\n","\n","print(f\"\\n予測値と残差:\")\n","print(f\"予測値の例 (最初の5個): {y_pred[:5]}\")\n","print(f\"残差の例 (最初の5個): {residuals[:5]}\")\n","\n","# ===============================\n","# 4. 分散分析とR²の手計算\n","# ===============================\n","# TSS (Total Sum of Squares) = Σ(yi - ȳ)²\n","TSS = np.sum((y - y_mean)**2)\n","\n","# ESS (Explained Sum of Squares) = Σ(ŷi - ȳ)²\n","ESS = np.sum((y_pred - y_mean)**2)\n","\n","# RSS (Residual Sum of Squares) = Σ(yi - ŷi)²\n","RSS = np.sum(residuals**2)\n","\n","# R² = ESS / TSS = 1 - RSS / TSS\n","r_squared = ESS / TSS\n","r_squared_alt = 1 - RSS / TSS\n","\n","# Adjusted R² = 1 - (RSS/(n-2)) / (TSS/(n-1))\n","adj_r_squared = 1 - (RSS/(n-2)) / (TSS/(n-1))\n","\n","print(f\"\\n分散分析:\")\n","print(f\"TSS (Total Sum of Squares): {TSS:.4f}\")\n","print(f\"ESS (Explained Sum of Squares): {ESS:.4f}\")\n","print(f\"RSS (Residual Sum of Squares): {RSS:.4f}\")\n","print(f\"R-squared: {r_squared:.4f}\")\n","print(f\"Adjusted R-squared: {adj_r_squared:.4f}\")"],"metadata":{"id":"IM8NfrXXf5Av"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================\n","# 5. 標準誤差の手計算\n","# ===============================\n","# 残差標準誤差 (RSE)\n","df_residual = n - 2\n","MSE = RSS / df_residual\n","RSE = np.sqrt(MSE)\n","\n","# β1の標準誤差\n","se_beta1 = RSE / np.sqrt(np.sum((x - x_mean)**2))\n","\n","# β0の標準誤差\n","se_beta0 = RSE * np.sqrt(1/n + x_mean**2 / np.sum((x - x_mean)**2))\n","\n","print(f\"\\n標準誤差:\")\n","print(f\"残差標準誤差 (RSE): {RSE:.4f}\")\n","print(f\"β1の標準誤差: {se_beta1:.4f}\")\n","print(f\"β0の標準誤差: {se_beta0:.4f}\")"],"metadata":{"id":"T-R5lURmgAes"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================\n","# 6. t検定統計量とp値の手計算\n","# ===============================\n","# t統計量\n","t_beta1 = beta1 / se_beta1\n","t_beta0 = beta0 / se_beta0\n","\n","# p値 (両側検定)\n","p_beta1 = 2 * (1 - stats.t.cdf(abs(t_beta1), df_residual))\n","p_beta0 = 2 * (1 - stats.t.cdf(abs(t_beta0), df_residual))\n","\n","# 信頼区間 (95%)\n","t_critical = stats.t.ppf(0.975, df_residual)\n","ci_beta1_lower = beta1 - t_critical * se_beta1\n","ci_beta1_upper = beta1 + t_critical * se_beta1\n","ci_beta0_lower = beta0 - t_critical * se_beta0\n","ci_beta0_upper = beta0 + t_critical * se_beta0\n","\n","print(f\"\\nt検定:\")\n","print(f\"t統計量 (β1): {t_beta1:.4f}\")\n","print(f\"t統計量 (β0): {t_beta0:.4f}\")\n","print(f\"p値 (β1): {p_beta1:.6f}\")\n","print(f\"p値 (β0): {p_beta0:.6f}\")\n","print(f\"95%信頼区間 (β1): [{ci_beta1_lower:.4f}, {ci_beta1_upper:.4f}]\")\n","print(f\"95%信頼区間 (β0): [{ci_beta0_lower:.4f}, {ci_beta0_upper:.4f}]\")"],"metadata":{"id":"HswZ2lbBgTOZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================\n","# 7. F統計量の手計算\n","# ===============================\n","# F統計量 = MSR / MSE = (ESS/1) / (RSS/(n-2))\n","F_statistic = (ESS / 1) / (RSS / df_residual)\n","p_F = 1 - stats.f.cdf(F_statistic, 1, df_residual)\n","\n","print(f\"\\nF検定:\")\n","print(f\"F統計量: {F_statistic:.4f}\")\n","print(f\"F統計量のp値: {p_F:.6f}\")"],"metadata":{"id":"efv76yq8gzj5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# \"5D\" graph plot"],"metadata":{"id":"cjzTZVXaX2vR"}},{"cell_type":"code","source":["# \"5D\" graph plot\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# データの生成\n","# X, Y, Z は空間座標\n","# Color_dim は4番目の次元 (例: 温度)\n","# Size_dim は5番目の次元 (例: 湿度)\n","num_points = 100\n","x = np.random.rand(num_points) * 10\n","y = np.random.rand(num_points) * 10\n","z = np.random.rand(num_points) * 10\n","color_dim = np.random.rand(num_points) * 100 # 例: 温度 (0-100)\n","size_dim = np.random.rand(num_points) * 50 + 10 # 例: 湿度 (10-60), sは点の面積なので適度に調整\n","\n","# 3Dプロットの設定\n","fig = plt.figure(figsize=(12, 10))\n","ax = fig.add_subplot(111, projection='3d')\n","\n","# 散布図のプロット\n","# c=color_dim で色が4番目の次元を表す\n","# s=size_dim でサイズが5番目の次元を表す\n","scatter = ax.scatter(x, y, z, c=color_dim, cmap='plasma', s=size_dim, alpha=0.7)\n","\n","# カラーバーの追加 (4番目の次元のスケールを表示)\n","cbar = fig.colorbar(scatter, ax=ax, pad=0.1)\n","cbar.set_label('Temperature (°C)')\n","\n","# 5番目の次元 (サイズ) の凡例は自動では表示されないため、手動でヒントを記述\n","# 例えば、異なるサイズのダミー点をいくつかプロットして凡例として使う方法もありますが、ここでは説明文で補足\n","ax.text2D(0.05, 0.95, \"Point Size represents Humidity\", transform=ax.transAxes)\n","\n","# ラベルの設定\n","ax.set_xlabel('X-axis')\n","ax.set_ylabel('Y-axis')\n","ax.set_zlabel('Z-axis')\n","ax.set_title('5D Plot: X, Y, Z by Position, 4th Dim by Color, 5th Dim by Size')\n","\n","plt.show()"],"metadata":{"id":"0MgapxiGSAcW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# F分布、上側確率が𝛂となる値を上側確率100𝛂％の**パーセント点**という"],"metadata":{"id":"RVnmmxPfYC1W"}},{"cell_type":"code","source":["from scipy.stats import f\n","\n","# 自由度を定義\n","df1 = 5  # 自由度 (m - 1)\n","df2 = 3 # 自由度 (n - 1)\n","\n","# 上側確率\n","alpha = 0.05\n","\n","# 上側パーセント点\n","f_critical = f.ppf(1 - alpha, df1, df2)\n","\n","print(f\"F({df1}, {df2})の上側{int(alpha*100)}%点: {f_critical}\")"],"metadata":{"id":"IBtLjsidYAVE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# lecture note 11の表12.2の関数で計算と手計算"],"metadata":{"id":"8jj9e_2MZjeY"}},{"cell_type":"code","source":["# 二項分布Bi(20,0.5)\n","from scipy.stats import binom\n","from matplotlib import pyplot as plt\n","\n","n = 20  #試行回数\n","p = 0.5  #成功確率\n","\n","x = list(range(n + 1))\n","\n","mean, var = binom.stats(n, p)  #平均と分散\n","\n","# list of 確率質量関数(pmf) values\n","dist = [binom.pmf(r, n, p) for r in x ]\n","# dist = binom(n, p).pmf(x) #or\n","\n","cdf = [binom.cdf(r, n, p) for r in x ]\n","\n","print(\"-\"*50)\n","print(\"x \\t f(x) \\t F(x)\") #\\t:a horizontal tab character\n","for i in range(n + 1):\n","    print(str(x[i]) + \"\\t\" + str(round(dist[i], 5))+ \"\\t\" + str(round(cdf[i], 5)))\n","\n","print(\"-\"*50)\n","print(\"平均mean = {}\".format(round(mean, 5)))\n","print(\"分散variance = {}\".format(round(var, 5)))\n","\n","plt.bar(x, dist, width = 0.2)"],"metadata":{"id":"CbeBi8pr3oGK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math\n","math.comb(20,14)*(1/2)**14*(1/2)**6+math.comb(20,15)*(1/2)**15*(1/2)**5+\\\n","  math.comb(20,16)*(1/2)**16*(1/2)**4+math.comb(20,17)*(1/2)**17*(1/2)**3+\\\n","  math.comb(20,18)*(1/2)**18*(1/2)**2+math.comb(20,19)*(1/2)**19*(1/2)**1+math.comb(20,20)*(1/2)**20*(1/2)**0"],"metadata":{"id":"u4lzKl6yeFcA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# lecture note 11の例：投薬群と対照群の平均の差の検定ーー手計算と関数で計算\n"],"metadata":{"id":"AYymaaa_biki"}},{"cell_type":"code","source":["# 手計算\n","# 等分散と仮定した場合\n","\n","import numpy as np\n","\n","G_T=np.array([7.97,7.66,7.59,8.44,8.05,8.08,8.35,7.77,7.98,8.15])\n","G_C=np.array([8.06,8.27,8.45,8.05,8.51,8.14,8.09,8.15,8.16,8.42])"],"metadata":{"id":"yeLcwjfKg_AG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["G_T.var(ddof=1)"],"metadata":{"id":"Ncp8-mrlRlBJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["G_C.var(ddof=1)"],"metadata":{"id":"Qtk0mgHpRzjW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["G_T.mean()"],"metadata":{"id":"Ejd1WM0fR0Wl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["G_C.mean()"],"metadata":{"id":"7zHkYtSQTEOd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# s_p^2　合併分散\n","(9*0.0761+9*0.0294)/18"],"metadata":{"id":"8zB23nrkTFsu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# s_p　合併標準偏差\n","np.sqrt((9*0.0761+9*0.0294)/18)"],"metadata":{"id":"inB5PdDgTV2_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# t値計算\n","(8.004-8.230)/(0.230*np.sqrt(0.1+0.1))"],"metadata":{"id":"M-k3FvuHTcLW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# t_{0.05}(18)左片側検定の臨界値計算\n","from scipy.stats import t\n","\n","# 上側確率\n","alpha = 0.05\n","\n","# 上側確率パーセント点\n","t_critical = t.ppf(1 - alpha, 18)\n","\n","t_critical"],"metadata":{"id":"oVUHhqYiToZG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ttest_ind関数で計算\n","from scipy.stats import ttest_ind #平均値の差のt検定-ttest_ind()を使う\n","\n","t_stat, p_value = ttest_ind(G_T, G_C, equal_var=True, alternative='less') #等分散と仮定したので、equal_var=True。また左片側検定なので、less；両側検定なら、'two-sided'、右片側検定なら、'greater'\n","\n","print(f\"t統計量: {t_stat:.4f}, p値: {p_value:.4e}\")"],"metadata":{"id":"yUYR3Ba4UDEm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yw-cuaOjQygV"},"source":["# 第8部　正規線形モデル"]},{"cell_type":"markdown","source":["## 1章　連続型の説明変数を1つ持つモデル"],"metadata":{"id":"5ARR4pr-RU1Z"}},{"cell_type":"markdown","metadata":{"id":"FJblC8l_QygZ"},"source":["### 実装：分析の準備"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yflODe6fQygZ"},"outputs":[],"source":["# 数値計算に使うライブラリ\n","import numpy as np\n","import pandas as pd\n","from scipy import stats\n","# 表示桁数の設定\n","pd.set_option('display.precision', 3)\n","np.set_printoptions(precision=3)\n","\n","# グラフを描画するライブラリ\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","sns.set()\n","\n","# 統計モデルを推定するライブラリ\n","import statsmodels.formula.api as smf\n","import statsmodels.api as sm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YbD0Dzn4Qyga"},"outputs":[],"source":["# 表示設定(書籍本文のレイアウトと合わせるためであり、必須ではありません)\n","np.set_printoptions(linewidth=60)\n","pd.set_option('display.width', 60)\n","\n","from matplotlib.pylab import rcParams\n","rcParams['figure.figsize'] = 8, 4"]},{"cell_type":"markdown","metadata":{"id":"WAVlk5G2Qygb"},"source":["### 実装：データの読み込みと図示"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"_J7aC42cQygb"},"outputs":[],"source":["# データの読み込み\n","beer = pd.read_csv('8-1-1-beer.csv')\n","print(beer.head(n=3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ogXKlmUtQygc"},"outputs":[],"source":["# 図示\n","sns.scatterplot(x='temperature', y='beer',\n","                data=beer, color='black')"]},{"cell_type":"markdown","metadata":{"id":"XGrdxJc1Qygc"},"source":["### 実装：係数の推定"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZMcv1muKQygd"},"outputs":[],"source":["# データの準備\n","x = beer['temperature']\n","y = beer['beer']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4FeHIkYyQygd"},"outputs":[],"source":["# 分散共分散行列\n","cov_mat = np.cov(x, y, ddof=0)\n","cov_mat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UEQQykWHQyge"},"outputs":[],"source":["# 係数の推定\n","\n","# 平均値\n","x_bar = np.mean(x)\n","y_bar = np.mean(y)\n","\n","# 共分散と分散\n","cov_xy =  cov_mat[0, 1]\n","s2_x = cov_mat[0, 0]\n","\n","# 係数の推定\n","beta_1 = cov_xy / s2_x\n","beta_0 = y_bar - beta_1 * x_bar\n","\n","print('切片      : ', round(beta_0, 3))\n","print('気温の係数: ', round(beta_1, 3))"]},{"cell_type":"markdown","metadata":{"id":"ima4T-inQyge"},"source":["### 実装：statsmodelsによるモデル化"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vrz_X_8WQyge"},"outputs":[],"source":["# モデルの構築\n","lm_model = smf.ols(formula='beer ~ temperature',\n","                   data=beer).fit()"]},{"cell_type":"markdown","metadata":{"id":"KLimUNK2Qyge"},"source":["### 実装：推定結果の表示と係数の検定"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qMVGiRu8Qyge"},"outputs":[],"source":["# 推定結果の表示\n","lm_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"TMtB0kCSQygf"},"source":["### 実装：AICによるモデル選択"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XSKuojkbQygf"},"outputs":[],"source":["# NULLモデル\n","null_model = smf.ols(formula='beer ~ 1', data=beer).fit()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ex6E0SBFQygf"},"outputs":[],"source":["# NULLモデルのAIC\n","round(null_model.aic, 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gr8MqKCdQygf"},"outputs":[],"source":["# 説明変数入りのモデルのAIC\n","round(lm_model.aic, 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SiJYc9aHQygf"},"outputs":[],"source":["# 対数尤度\n","round(lm_model.llf, 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TbyhZsbEQygf"},"outputs":[],"source":["# 説明変数の数\n","lm_model.df_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cpIx9icjQygg"},"outputs":[],"source":["# AIC\n","round(-2 * (lm_model.llf - (lm_model.df_model + 1)), 3)"]},{"cell_type":"markdown","metadata":{"id":"peoeNZb7Qygg"},"source":["### 実装：単回帰による予測"]},{"cell_type":"markdown","metadata":{"id":"bfDjV8kPQygg"},"source":["#### 当てはめ値"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iWaAcs1uQygg"},"outputs":[],"source":["# 当てはめ値\n","lm_model.predict()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WIJlaqkDQygg"},"outputs":[],"source":["# 参考：当てはめ値を取得する別の方法(書籍には載っていないコードです)\n","lm_model.fittedvalues"]},{"cell_type":"markdown","metadata":{"id":"f6dhao5cQygg"},"source":["#### 気温が0度のときの予測値"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"psyldu0mQygg"},"outputs":[],"source":["# 予測\n","lm_model.predict(pd.DataFrame({'temperature':[0]}))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QRaXgQbsQygh"},"outputs":[],"source":["# 気温0どの時の予測値は切片に等しい\n","lm_model.params"]},{"cell_type":"markdown","metadata":{"id":"ZPdjaMotQygh"},"source":["#### 気温が20度のときの予測値"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tpb5n0vhQygh"},"outputs":[],"source":["# 予測\n","lm_model.predict(pd.DataFrame({'temperature':[20]}))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JiGAFeOcQygh"},"outputs":[],"source":["# predict関数を使わないで予測\n","beta0 = lm_model.params[0]\n","beta1 = lm_model.params[1]\n","temperature = 20\n","\n","round(beta0 + beta1 * temperature, 3)"]},{"cell_type":"markdown","metadata":{"id":"dH_x1uc6Qygh"},"source":["### 実装：信頼区間・予測区間"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VMx5dAEQQygh"},"outputs":[],"source":["# 当てはめ結果の信頼区間と予測区間を得る\n","pred_interval = lm_model.get_prediction(\n","    pd.DataFrame({'temperature':[20]}))\n","pred_frame = pred_interval.summary_frame(alpha=0.05)\n","print(pred_frame)"]},{"cell_type":"markdown","metadata":{"id":"1HYIpUxQQygi"},"source":["### 実装：seabornによる回帰直線の図示"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xk2JbnWAQygi"},"outputs":[],"source":["sns.lmplot(x='temperature', y='beer', data=beer,\n","           scatter_kws={'color': 'black'},\n","           line_kws   ={'color': 'black'},\n","           ci=None, height=4, aspect=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"TGsqo5JvQygi"},"outputs":[],"source":["# 参考：axis-level関数(書籍には載っていないコードです)\n","sns.regplot(x='temperature', y='beer', data=beer,\n","           scatter_kws={'color': 'black'},\n","           line_kws   ={'color': 'black'},\n","           ci=None)"]},{"cell_type":"markdown","metadata":{"id":"brra1hqXQygi"},"source":["### 実装：信頼区間と予測区間の図示"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"whq3CN7kQygi"},"outputs":[],"source":["# 当てはめ結果の信頼区間と予測区間を得る\n","pred_all = lm_model.get_prediction()\n","pred_frame_all = pred_all.summary_frame(alpha=0.05)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j8wq6Ef6Qygm"},"outputs":[],"source":["# 説明変数を付け加える\n","pred_graph = pd.concat(\n","    [beer.temperature, pred_frame_all], axis = 1)\n","\n","# 図示のためにソートする\n","pred_graph = pred_graph.sort_values(\"temperature\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H63jxyCIQygm"},"outputs":[],"source":["# 参考：グラフ描画用のデータにおける、最初の3行(書籍には載っていないコードです)\n","pred_graph.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DtJL2e8rQygn"},"outputs":[],"source":["# 散布図\n","sns.scatterplot(x='temperature', y='beer',\n","                data=beer, color='black')\n","\n","# 回帰直線\n","sns.lineplot(x='temperature', y='mean',\n","             data=pred_graph, color='black')\n","\n","# 信頼区間\n","sns.lineplot(x='temperature', y='mean_ci_lower',\n","             data=pred_graph, color='black',\n","             linestyle='dashed')\n","sns.lineplot(x='temperature', y='mean_ci_upper',\n","             data=pred_graph, color='black',\n","             linestyle='dashed')\n","\n","# 予測区間\n","sns.lineplot(x='temperature', y='obs_ci_lower',\n","             data=pred_graph, color='black',\n","             linestyle='dotted')\n","sns.lineplot(x='temperature', y='obs_ci_upper',\n","             data=pred_graph, color='black',\n","             linestyle='dotted')"]},{"cell_type":"markdown","metadata":{"id":"oR91tFjHQ9yg"},"source":["## 2章　正規線形モデルの評価"]},{"cell_type":"markdown","metadata":{"id":"vFWuvyKTQ9yi"},"source":["### 分析の準備"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"upSR_72rQ9yj"},"outputs":[],"source":["# 数値計算に使うライブラリ\n","import numpy as np\n","import pandas as pd\n","from scipy import stats\n","# 表示桁数の設定\n","pd.set_option('display.precision', 3)\n","np.set_printoptions(precision=3)\n","\n","# グラフを描画するライブラリ\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","sns.set()\n","\n","# 統計モデルを推定するライブラリ\n","import statsmodels.formula.api as smf\n","import statsmodels.api as sm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o1qGfE04Q9yl"},"outputs":[],"source":["# 表示設定(書籍本文のレイアウトと合わせるためであり、必須ではありません)\n","np.set_printoptions(linewidth=60)\n","pd.set_option('display.width', 60)\n","\n","from matplotlib.pylab import rcParams\n","rcParams['figure.figsize'] = 8, 4"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"MBAEAOmYQ9yl"},"outputs":[],"source":["# データの読み込み\n","beer = pd.read_csv('8-1-1-beer.csv')\n","\n","# モデル化\n","lm_model = smf.ols(formula='beer ~ temperature',\n","                   data=beer).fit()"]},{"cell_type":"markdown","metadata":{"id":"6Z9cBACiQ9ym"},"source":["### 実装：残差の取得"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Ks2Pe2HQ9ym"},"outputs":[],"source":["# 残差の取得\n","e = lm_model.resid\n","e.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1xCgL6ljQ9yn"},"outputs":[],"source":["# 当てはめ値の計算\n","beta0 = lm_model.params[0] # 切片\n","beta1 = lm_model.params[1] # 傾き\n","\n","y_hat = beta0 + beta1 * beer.temperature # 当てはめ値\n","y_hat.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HLRe1u2Q9yn"},"outputs":[],"source":["# 参考：当てはめ値の取得(書籍には載っていないコードです)\n","lm_model.fittedvalues.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4tBkXhthQ9yo"},"outputs":[],"source":["# 当てはめ値から残差を求める\n","(beer.beer - y_hat).head(3)"]},{"cell_type":"markdown","metadata":{"id":"DpPDypCqQ9yo"},"source":["### 実装：決定係数"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dVsx9OHLQ9yp"},"outputs":[],"source":["# 決定係数\n","y = beer.beer              # 応答変数y\n","y_bar = np.mean(y)         # yの平均値\n","y_hat = lm_model.predict() # yの当てはめ値\n","\n","round(np.sum((y_hat - y_bar)**2) / np.sum((y - y_bar)**2), 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VfgtufDzQ9yp"},"outputs":[],"source":["round(lm_model.rsquared, 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_11oUH5gQ9yp"},"outputs":[],"source":["round(np.sum((y_hat - y_bar)**2) + sum(e**2), 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4DC6B8zgQ9yq"},"outputs":[],"source":["round(np.sum((y - y_bar)**2), 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ej-FbBA1Q9yq"},"outputs":[],"source":["# 決定係数の別の求め方\n","round(1 - np.sum(e**2) / np.sum((y - y_bar)**2), 3)"]},{"cell_type":"markdown","metadata":{"id":"8PrjDSqjQ9yq"},"source":["### 実装：自由度調整済み決定係数"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CBde-cceQ9yq"},"outputs":[],"source":["n = len(beer.beer) # サンプルサイズ\n","d = 1              # 説明変数の数\n","r2_adj = 1 - ((np.sum(e**2) / (n - d - 1)) /\n","    (np.sum((y - y_bar)**2) / (n - 1)))\n","round(r2_adj, 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"glVNZGwwQ9yq"},"outputs":[],"source":["round(lm_model.rsquared_adj, 3)"]},{"cell_type":"markdown","metadata":{"id":"57zX-amgQ9yr"},"source":["### 実装：残差の可視化"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IYKWWKNWQ9yr"},"outputs":[],"source":["# 残差のヒストグラム\n","sns.histplot(e, color='gray')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C2jAbNkXQ9yr"},"outputs":[],"source":["# 残差の散布図\n","sns.residplot(x=lm_model.fittedvalues, y=e, color='black')"]},{"cell_type":"markdown","metadata":{"id":"VnI0o3M3Q9yr"},"source":["### 実装：Q-Qプロット"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FDJpsmcUQ9yr"},"outputs":[],"source":["# ワーニングが出ることがありますが、動作に影響はありません\n","# やや煩わしいので、ワーニングを出さない設定をしておきました\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"meEI1ytJQ9yr"},"outputs":[],"source":["# Q-Qプロット\n","fig = sm.qqplot(e, line='s')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ierHO9MlQ9yr"},"outputs":[],"source":["# 小さいものから順番に並び替え\n","e_sort = e.sort_values()\n","e_sort.head(n=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PISYkr8MQ9yr"},"outputs":[],"source":["# 最も小さな値は全体の下位●％に位置する\n","round(1 / 31, 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kjb6nnR2Q9ys"},"outputs":[],"source":["# サンプルサイズだけ、0~1の範囲を区切る\n","# これが理論上の累積確率となる\n","nobs = len(e_sort)\n","cdf = np.arange(1, nobs + 1) / (nobs + 1)\n","cdf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"78TG_rGeQ9ys"},"outputs":[],"source":["# 累積確率に対応するパーセント点\n","ppf = stats.norm.ppf(cdf)\n","ppf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"787AaLSsQ9ys"},"outputs":[],"source":["# 参考：理論上のパーセント点と実際の残差をプロットするとQ-Qプロットになる(書籍には載っていないコードです)\n","sns.scatterplot(x=ppf, y=e_sort,  color='black')"]},{"cell_type":"markdown","metadata":{"id":"mHhShpTKQ9ys"},"source":["### 実装：summary関数の出力で見る残差のチェック"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yof73jyLQ9ys"},"outputs":[],"source":["# 推定結果の表示\n","lm_model.summary()"]},{"cell_type":"markdown","source":["# 下記は「東京大学のデータサイエンティスト育成講座」から"],"metadata":{"id":"NGyFoGvhTaoA"}},{"cell_type":"markdown","metadata":{"id":"wOhhv2LvQ-xd"},"source":["# 8 機械学習の基礎（教師あり学習）"]},{"cell_type":"markdown","metadata":{"id":"SJxkE3wtQ-xe"},"source":["機械学習は、何かしらの目的を達成するための知識や行動を、データを読み込ませることで機械に獲得させるための技術です。機械学習は大きく、教師あり学習、教師なし学習、強化学習に分けられ、この章では、教師あり学習の具体的手法について学びます。この章を通し、機械学習の考え方とモデル構築の基本作法を理解し、正しく実行できるようになりましょう。\n","\n","\n","ゴール：機械学習の体系と概要を学び、教師あり学習のモデル（重回帰分析、ロジスティック回帰分析、、リッジ回帰、ラッソ回帰、決定木、k-NN、SVM）を使ってモデル構築や評価を正しく実行できるようになる"]},{"cell_type":"markdown","metadata":{"id":"E9WJOOb9Q-xg"},"source":["## 8.1 機械学習の全体像\n","ゴール：機械学習の体系と教師あり学習、教師なし学習の概要を知る"]},{"cell_type":"markdown","metadata":{"id":"QBKaTEHTQ-xh"},"source":["この章では、教師あり学習の具体的手法について学びます。教師あり学習は、機械学習の中で最もビジネス活用が進んでいる技術です。この章を通し、機械学習の考え方とモデル構築の基本作法を理解し、正しく実行できるようになりましょう。\n","\n","教師あり学習の話に入る前に、教師なし学習なども含めて、まずは、機械学習の全体像を俯瞰してみましょう。"]},{"cell_type":"markdown","metadata":{"id":"hYcqEyPaQ-xh"},"source":["### 8.1.1 機械学習とは\n","キーワード：機械学習、教師あり学習、教師なし学習、強化学習"]},{"cell_type":"markdown","metadata":{"id":"siyDQ-tXQ-xi"},"source":["**機械学習（machine learning）**は、何かしらの目的を達成するための知識や行動を、データを読み込ませることで機械に獲得させるための技術です。機械学習は大きく、**教師あり学習（supervised learning）**、**教師なし学習（unsupervised learning）**、**強化学習（reinforcement learning）**に分けられます。この分け方以外にも、教師あり学習と教師なし学習の2つに分けたり、これらの3つに、さらに半教師あり学習を加えて4つに分けることもあります。"]},{"cell_type":"markdown","metadata":{"id":"-J37Wpe9Q-xi"},"source":["#### 教師あり学習と教師なし学習\n","\n","機械に読み込ませて知識や行動を獲得させるために使うデータのことを訓練データと言います。\n","教師あり学習と教師なし学習の違いは、訓練データに、目的変数や説明変数（後述）があるかどうかです。端的に言うと、正解のデータがあってそれを与えるのが教師あり学習、そうでないのが教師なし学習です。"]},{"cell_type":"markdown","metadata":{"id":"NPNb16BXQ-xj"},"source":["①教師あり学習\n","\n","説明変数（インプット）から目的変数（アウトプット）を予測するモデルを求める手法です。訓練データには目的変数や説明変数があり、あらかじめ作ったモデルに訓練データの説明変数を入力し、そのモデルからの出力が訓練データの目的変数に近づくようにモデルのパラメータを調整することで学習していきます。この章で詳しく説明します。\n","\n","たとえば、メールのタイトルや内容（説明変数）からスパムか否か（目的変数）を識別したい、株の売買状況（説明変数）から株価（目的変数）を予測したいときなどに使われます。"]},{"cell_type":"markdown","metadata":{"id":"MZa_cCSkQ-xj"},"source":["②教師なし学習\n","\n","入力データそのものに着目し、データに潜むパターンや示唆を見いだす手法です。訓練データに目的変数や説明変数はありません。多数のデータをいくつかの類似グループに分けるクラスタリングや、データ次元（変数の数）を、元のデータの情報を失わないようにより少数の次元に縮約する主成分分析（PCA:Principle Component Analysis）などの手法があります。データに解釈を与える探索的分析やデータの次元圧縮（dimentional reduction）などに使われます。こちらは次の章で詳しく説明します。なお、次元圧縮は教師あり学習もありますが、次の章で扱うのは教師なし学習の次元圧縮です。"]},{"cell_type":"markdown","metadata":{"id":"qAWgwiS9Q-xk"},"source":["#### 強化学習\n","\n","強化学習は、プログラム（機械）の一連の行動の結果に対して報酬を与えることで、機械に実現させたい知識や行動ルールを獲得させようとする手法です。教師あり学習では1つ1つの行動に正解データを付与する必要がありますが、たとえば対戦相手のいるゲームなど相互作用的な環境下で行動ルールを獲得する必要がある場合、すべての局面に対して正解データを付与することは困難です。\n","\n","そのため、一連の行動の結果に基づく報酬によって行動ルールを獲得させようとする強化学習は、教師あり学習では表現困難なケースを扱える学習方式として、近年注目を集めています。"]},{"cell_type":"markdown","metadata":{"id":"aWy3wVCqQ-xk"},"source":["#### 機械学習を適用するにあたって\n","\n","本書では、与えられたデータに対して機械学習を適用するところから始めますが、実際のデータ分析の現場においては、「基本統計量を取得する」「ヒストグラムや散布図を作成する」など、データの基本的な観察と理解を怠らないようにしましょう。データの品質は機械学習のアウトプットの品質にも大きな影響を与えるためです。また、そのような一連の確認作業からデータ上の有益な気付きを得られることもあります。機械学習を使うことを目的とせず、あくまで1つの手段であることを留意しておきましょう。"]},{"cell_type":"markdown","metadata":{"id":"zyNL_uDLQ-xk"},"source":[">**[ポイント]**\n",">\n",">現場でデータ分析をするときは、機械学習を適応する前に、基本統計量や散布図を作成し、データの傾向や全体像を抑えましょう。"]},{"cell_type":"markdown","metadata":{"id":"FCNWwjlNQ-xk"},"source":["機械学習の入門者には、参考文献「A-18」や参考URL「B-24」が参考になるでしょう。"]},{"cell_type":"markdown","metadata":{"id":"Z0o-MFCKQ-xk"},"source":["ビジネス的な視点で機械学習を活かすことを学ぶには参考文献「A-19」などが参考になります。\n","\n","また、1冊の専門書で機械学習のモデルや実装に関する情報をすべて記載するのは無理なので、何かわからないこと（パラメータの設定など）があった場合、大事になるのは公式ドキュメント(参考URL「B-27」)\n","に戻ることです。公式ドキュメントはぎっしり書いてあってなかなか読み切れるものではないですが、モデルの細かなパラメータ等の説明がありますので、確実です。"]},{"cell_type":"markdown","metadata":{"id":"f4djgsIzQ-xl"},"source":[">**[ポイント]**\n",">\n",">機械学習等のモデルでパラメータやモデルの特性などわからないことがあれば、まずは公式ドキュメントを調べましょう。"]},{"cell_type":"markdown","metadata":{"id":"8PlTeX5PQ-xl"},"source":["### 8.1.2 教師あり学習\n","キーワード：目的変数、説明変数、回帰、分類"]},{"cell_type":"markdown","metadata":{"id":"UvrvsOCJQ-xl"},"source":["教師あり学習は、訓練データを与えて、そこに含まれる正解データを予測するためのモデルを構築する手法です。先に説明したとおり、訓練データの中で予測したい変数のことを**目的変数**（他には正解データ、応答変数、ターゲット変数、従属変数などとも呼ばれます）、目的変数を説明するための変数のことを**説明変数**と言います（他には特徴量、予測変数、独立変数などとも呼ばれます）。\n","\n","$y=f(x)$という関数があるとすると、$y$が目的変数、$x$が説明変数、関数$f(x)$がモデルです。たとえば、ある消費財ブランドの購買者が、将来ブランド非購買になるか否か（目的変数）を予測したいときは、過去のさまざまなデータ（顧客属性、購買頻度、関連ブランドの購入有無など）を説明変数として扱います。"]},{"cell_type":"markdown","metadata":{"id":"dq0ErXggQ-xl"},"source":["#### 教師あり学習の手法\n","\n","教師あり学習は目的変数のデータ形式によって、いくつかの種類に分類できます。目的変数が株価など数値を取る場合を**回帰(regression)**、「男性・女性」「幼児・小学生・学生・大人」などのカテゴリになる場合を**分類（classification）**といいます。たとえば先ほどのブランド非購買になるか否かのケースは、「購入する」か「購入しないか」の2つのカテゴリに分ける分類タスクです。"]},{"cell_type":"markdown","metadata":{"id":"WJB9kzgPQ-xl"},"source":["教師あり学習のアルゴリズム（手法）には、**重回帰（multiple linear regression）、ロジスティック回帰（logistic regression）、k近傍法（k-Nearest Neighbors）、決定木（Decision Tree）、サポートベクターマシン（Support Vector Machine）、ランダムフォレスト（Random Forest）、勾配ブースティング（Gradient Boosting）**等があります。これらの手法は、回帰で使われるときもあれば、分類で使われるときもあるので、注意しましょう。\n","\n","ちなみに、ロジスティック回帰は回帰という名前がついていますが、分類の用途で使われます。決定木は一般的に分類に使う手法ですがこの場合は分類木といい、回帰の場合は回帰木といいます。後に個別に説明します。"]},{"cell_type":"markdown","metadata":{"id":"Qe6neQlXQ-xl"},"source":["どの手法を選択するのかは、求めるモデルの性能で決めるのが基本です。しかし学習結果の解釈性（interpretability/解釈しやすさ）を優先したい場合は重回帰、ロジスティック回帰、決定木などの比較的シンプルな手法を意図的に採用することもあります。サポートベクターマシンなどは説明がしにくく、非専門家が1回聞いてすぐに理解できる手法ではないためです（機械学習で「決定木」は理解しやすいという記載もありますが、非専門家にとっては必ずしもわかりやすい概念ではないことを留意しておきましょう）。解釈性を優先すべき局面なのか、解釈よりも精度を追求すべき局面なのかについて、ケースバイケースで判断するようにしましょう。"]},{"cell_type":"markdown","metadata":{"id":"kYeFxZocQ-xl"},"source":["### 8.1.3 教師なし学習\n","キーワード：クラスタリング、主成分分析、マーケットバスケット分析"]},{"cell_type":"markdown","metadata":{"id":"avX5QfTIQ-xl"},"source":["教師なし学習は目的変数がなく入力データそのものに注目した学習で、データに潜むパターンや示唆を見出そうとするものです。"]},{"cell_type":"markdown","metadata":{"id":"BT7Zx3MRQ-xm"},"source":["#### 教師なし学習の手法\n","\n","教師なし学習の代表的な手法が、多数のデータをいくつかの類似のグループに分ける**クラスタリング（clustering）**です。たとえば、ある消費者がどのような嗜好グループに分かれるかといったマーケティング分析などに使われます。"]},{"cell_type":"markdown","metadata":{"id":"ngmP74ALQ-xm"},"source":["クラスタリングは、データそのものの特徴を探す手法であることから、探索的なデータ分析手法とも位置づけられます。クラスタリング結果に基づき対象データをグルーピングをしたら終わりではなく、そこに解釈を与えそれがビジネスなどの現場感覚とズレていないかを確認することは重要です。探索的なデータ分析では完全な自動化は難しく、人の判断が重要な役割を担うことを留意しましょう。"]},{"cell_type":"markdown","metadata":{"id":"XRLaqBITQ-xm"},"source":["教師なし学習にはほかにも、**主成分分析（Principle Component Analysis）**や**マーケットバスケット分析（Market Basket Analysis）**などがあります。主成分分析は、多数の変数をそれらの情報を失わないように縮約して、変数を減らす分析手法です。マーケットバスケット分析はPOS（Point of Sales）といわれる購買データ等の分析に使われ、ある商品Aを買っている人は高い確率である商品Bも買っている、といったアソシエーションルール（関連性の強い事象の組み合わせのこと）を求めてくれる分析手法です。"]},{"cell_type":"markdown","metadata":{"id":"wEpLzs_vQ-xm"},"source":["### 8.1.4 強化学習\n","キーワード：動的計画法、モンテカルロ法、TD学習"]},{"cell_type":"markdown","metadata":{"id":"qFRM92yeQ-xm"},"source":["強化学習は、ある報酬を最大化するために何をすべきかの行動ルールを、機械に学習させるための技術です。報酬は機械の一連の行動の結果に対し目的と整合するように設計します。つまり望ましい結果には高い報酬を、望ましくない結果には低い報酬を与えるようにします。教師あり学習のように1つ1つの行動に対する正解データは与えられず、その代わりどのような行動を取ったら最終的により大きな報酬を得られるかを見つけ出そうとします。強化学習では、機械（エージェント）が存在する環境や他のエージェントとの相互作用の中で学習が進みます。"]},{"cell_type":"markdown","metadata":{"id":"AmLoEwhbQ-xm"},"source":["実例で言うとたとえば、「赤ちゃん（エージェント）は歩き方を教わっていないのに自分がおかれている環境の中から試行錯誤しながら歩けるようになる」「自動車（エージェント）が他の自動車（他のエージェント）と衝突することなく走行できるようになる」などが、強化学習の例となります。"]},{"cell_type":"markdown","metadata":{"id":"FUsu6n0IQ-xn"},"source":["#### 強化学習の手法\n","\n","強化学習ではエージェントが探索的に行動し、環境との相互作用の中から学習が進むため、探索と知識利用のジレンマ（Exploration-Exploitation Dilemma）をどのように扱うかが重要なテーマです。これは、過去の行動から学んだ結果を踏まえて「一番良い行動」を取っていたら新しい行動を見つけられなくなるし（知識に偏る）、「もっと良い行動」を求めて新しい行動ばかりしていると過去の経験を活かせない（探索に偏る）ので、探索と知識利用のバランスをどうとるかが大切、ということです。"]},{"cell_type":"markdown","metadata":{"id":"pQw5iqz3Q-xn"},"source":["強化学習のアプローチには動的計画法、モンテカルロ法やTD学習などがあります。動的計画法は明示的な知識があることを前提としますが、モンテカルロ法は環境における完全な知識を必要とせず経験のみを必要とする方法です。なお本書では、強化学習については以上の概念の紹介までとします。さらに学習を深めたい方は、上記までに登場した用語を参考に、参考文献「A-21」や参考URL「B-26」のOpenAIのサイトなどを参照して下さい。"]},{"cell_type":"markdown","metadata":{"id":"FJf3JqC0Q-xn"},"source":["### 8.1.5 この章で使うライブラリのインポート\n"]},{"cell_type":"markdown","metadata":{"id":"2zslGipyQ-xn"},"source":["Scikit-learnは、単回帰分析の際にも使いました。Scikit-learnの公式ドキュメントには、詳細な仕様や使い方が記されているので、参考にしてください。Sciklt-learnのライブラリには、機械学習用のクラスだけでなくサンプルデータもいくつか含まれています。\n","\n","https://scikit-learn.org/stable/user_guide.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WMp6-c1_Q-xn"},"outputs":[],"source":["# データ加工・処理・分析ライブラリ\n","import numpy as np\n","import numpy.random as random\n","import scipy as sp\n","from pandas import Series, DataFrame\n","import pandas as pd\n","\n","# 可視化ライブラリ\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","import seaborn as sns\n","%matplotlib inline\n","\n","# 機械学習ライブラリ\n","import sklearn\n","\n","# 小数第3位まで表示\n","%precision 3"]},{"cell_type":"markdown","metadata":{"id":"U7ZoTfEFQ-xo"},"source":["## 8.2 重回帰\n","キーワード：目的変数、説明変数、多重共線性、変数選択法"]},{"cell_type":"markdown","metadata":{"id":"8MAO0HY7Q-xs"},"source":["教師あり学習の1つ目として、まずは、**重回帰（multiple lienar regression）**について学びます。単回帰では目的変数に対して説明変数は1つでした。この考え方を拡張し、説明変数が1つではなく複数ある場合を扱うのが重回帰です。重回帰によって、各説明変数の係数（回帰係数）が推定され予測値を計算できます。回帰係数は予測値と目的変数の2乗誤差が最小になるように推定されます。\n","\n","[](いまさらですが、改めて読み直したところ、3章で、「回帰係数とは何か」について、初出時に、きちんと定義しておいたほうがよいかとも思いました。3章では「**最小二乗法**という手法で回帰係数$a$と切片$b$を計算しています。」が初出でした)"]},{"cell_type":"markdown","metadata":{"id":"MABmdyKXQ-xt"},"source":["### 8.2.1 自動車価格データの取り込み\n","\n","それでは、実際にやってみましょう。ここでは、自動車の価格とそれらの属性（自動車の大きさなど）データがあるとき、その属性から自動車価格を予測するモデルを重回帰を使って構築してみましょう。\n","データは、次のURLで公開されているものを利用します。\n","\n","http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HWZP03DcQ-xt"},"outputs":[],"source":["# インポート\n","import requests, zipfile\n","import io\n","\n","# 自動車価格データを取得\n","url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data'\n","res = requests.get(url).content\n","\n","# 取得したデータをDataFrameオブジェクトとして読み込み\n","auto = pd.read_csv(io.StringIO(res.decode('utf-8')), header=None)\n","\n","# データの列にラベルを設定\n","auto.columns =['symboling','normalized-losses','make','fuel-type' ,'aspiration','num-of-doors',\n","                            'body-style','drive-wheels','engine-location','wheel-base','length','width','height',\n","                            'curb-weight','engine-type','num-of-cylinders','engine-size','fuel-system','bore',\n","                            'stroke','compression-ratio','horsepower','peak-rpm','city-mpg','highway-mpg','price']"]},{"cell_type":"markdown","metadata":{"id":"itxCJWKjQ-xt"},"source":["上記のプログラムを実行すると、変数`auto`にPandasの`DataFrame`オブジェクトとして、自動車価格データが設定されます。\n","実際に、どのようなデータなのか確認してみましょう。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hzNJe1vsQ-xt"},"outputs":[],"source":["print('自動車データの形式:{}'.format(auto.shape))"]},{"cell_type":"markdown","metadata":{"id":"v0S1EJTRQ-xu"},"source":["205行、26列のデータであることが分かります。"]},{"cell_type":"markdown","metadata":{"id":"lvpE3LXzQ-xu"},"source":["続いて次のように`head()`メソッドで、最初の5行を表示してみます。"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"JysLF75rQ-xu"},"outputs":[],"source":["auto.head()"]},{"cell_type":"markdown","metadata":{"id":"ixdvp2VUQ-xu"},"source":["このデータにおいて、自動車の価格は`price`に設定されています。ここでは自動車の属性から価格を予測するモデルを作ろうとしているのですから、`price`以外の値から`price`を予測するモデルを作るというのが課題となります。\n","\n","すべての説明変数から`price`を予測するのは複雑なので、ここでは、`horsepower`、`width`、`height`の3つの説明変数だけを使うものとします。つまり、`horsepower`、`width`、`height`という説明変数から`price`という目的変数を予測するというモデルを作成していくものとします。"]},{"cell_type":"markdown","metadata":{"id":"xkNelvRkQ-xu"},"source":["### 8.2.2 データの整理\n","\n","入力データには不適切なものが含まれていることがあります。そこでまずは、データの内容を確認して適切なデータとして整理します。"]},{"cell_type":"markdown","metadata":{"id":"ac43EF3PQ-xu"},"source":["#### 不適切なデータの除去\n","\n","先ほど`head()`を使ってデータを確認しましたが、このとき、データの中に'?'のあることに気づきます。多くの機械学習のアルゴリズムは、数値型データしか扱えないため、このような「?」などの非数値データを含む変数に対しては、それを取り除く前処理が必要です。\n","\n","今回の目的は`horsepower`、`width`、`height`から`price`を予測することなので、これらの変数に'?'データがあれば削除します。具体的には、'?'データを欠損値に変換をした上で欠損値を含む行を除外します。扱おうとしている`horsepower`、`width`、`height`、`price`の4つ変数に'?'データが、どれだけ含まれているのかは、次のプログラムで確認できます。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PfEucrJ-Q-xv"},"outputs":[],"source":["# それぞれのカラムに ? が何個あるかカウント\n","auto = auto[['price','horsepower','width','height']]\n","auto.isin(['?']).sum()"]},{"cell_type":"markdown","metadata":{"id":"6hrXPpVAQ-xv"},"source":["`price`と`horsepower`に'?'データが混入していることがわかるので、次のようにすると'?'がある行が除去されます。実行すると、行数が減っていることが確認できます。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rcxGIJIUQ-xv"},"outputs":[],"source":["# '?'をNaNに置換して、NaNがある行を削除\n","auto = auto.replace('?', np.nan).dropna()\n","print('自動車データの形式:{}'.format(auto.shape))"]},{"cell_type":"markdown","metadata":{"id":"JfkC0NnmQ-xw"},"source":["#### 型の変換\n","ここでデータの型を確認しておきましょう。次のようにして確認します。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IgpaqbTFQ-xw"},"outputs":[],"source":["print('データ型の確認（型変換前）\\n{}\\n'.format(auto.dtypes))"]},{"cell_type":"markdown","metadata":{"id":"ySh6xHS7Q-xw"},"source":["確認すると`price`と`horsepower`が数値型ではないことがわかります。そこで`to_numeric`を使って数値型に変換しておきます。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"veaZNb36Q-xx"},"outputs":[],"source":["auto = auto.assign(price=pd.to_numeric(auto.price))\n","auto = auto.assign(horsepower=pd.to_numeric(auto.horsepower))\n","print('データ型の確認（型変換後）\\n{}'.format(auto.dtypes))"]},{"cell_type":"markdown","metadata":{"id":"ns8cOFKSQ-xx"},"source":["#### 相関の確認\n","\n","以上の操作で、説明変数、目的変数のすべての行は、欠損が無くかつ数値型のデータ形式に加工されました。\n","続けて各変数の相関を確認します。次のように`corr`を使うと、相関を確認できます。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uEz-UxIhQ-xy"},"outputs":[],"source":["auto.corr()"]},{"cell_type":"markdown","metadata":{"id":"lmg2vLlFQ-xy"},"source":["`price`が今回の目的変数なので、それ以外の3変数に注目をすると、`width`と`horsepower`の相関が0.6程度と、やや高いことに気づきます。なぜこのような確認をしているかというと、相関の高い変数を同時に重回帰の説明変数とすると、**多重共線性（multi-collinearity）**が生じる可能性があるからです。\n","\n","\n","多重共線性とは変数間の高い相関のために回帰係数の分散が大きくなり、係数の有意性が失われてしまう現象です。このような現象を回避すべく、通常、重回帰のモデル構築においては、相関の高い変数群からは代表となる変数だけをモデルに使用します。しかしここでは実験なのでそこまで厳密に考えず、`width`と`horsepower`の両方を、`height`と一緒に残してモデル構築を進めることとします。\n","\n"]},{"cell_type":"markdown","metadata":{"id":"p7Mop813Q-xy"},"source":["### 8.2.3 モデル構築と評価\n","\n","データが揃ったのでモデルを構築してみましょう。重回帰のモデルを作り、その性能を調べるプログラムは下記のようになります。"]},{"cell_type":"markdown","metadata":{"id":"Er9wpGIVQ-xy"},"source":["下記のプログラムでは、説明変数を$X$、目的変数を$y$に設定しています。\n","\n","機械学習のモデル構築では、「モデル構築に使用する訓練データ」を使って学習させてモデルを構築し、そのモデルに訓練データとは別の「テストデータ」を入れて、テストデータに対して、どの程度の精度が得られるのかを確認することで性能を調べるのが一般的です。そこで以下では、Scikit-learnの`model_selection`モジュールの`train_test_split`関数を使い、訓練データとテストデータに分けています。"]},{"cell_type":"markdown","metadata":{"id":"ySE-fSW3Q-xz"},"source":["この関数はデータをランダムに2つに分ける関数です。どのような割合で分類するのかは`test_size`で決めます。ここでは`test_size`を0.5にしているので半分ずつに分かれます（たとえば、0.4にすると4対6に分けることもできます）。\n","\n","`random_state`は乱数の生成を制御するものです。ここでは`random_state`を0に設定しています。このように`random_state`を固定する（この場合は0に設定する）とランダム性がなくなり、何度実行しても、同じように分離されます。もし0でないと、実行のたびに、ある行が訓練データに分類されたりテストデータに分類されたりとまちまちになるので、結果が一定となりません。ですからモデル性能の実証段階では、`random_state`を固定して再現性を持たせることは、とても重要です。\n"]},{"cell_type":"markdown","metadata":{"id":"Yu9JtlLiQ-xz"},"source":["重回帰のモデル構築は、`LinearRegression`クラスを使って行います。「`model = LinearRegression()`」でインスタンスを作成して、訓練データを「`model.fit(X_train,y_train)`」のように読み込ませると学習が完了します。学習したら、決定係数や回帰係数、切片を確認できます。決定係数とは、目的変数によって予測された値が、実際の目的変数の値とどのくらい近いかを示す値です。"]},{"cell_type":"markdown","metadata":{"id":"oi_JPlYuQ-xz"},"source":["機械学習の目的は高い汎化性能の獲得（構築したモデルによって、未知データでも適切に予測できること）ですから、訓練データへのあてはまりを追求すれば良いモデルになりそうですが、実際はそうではなく、訓練データに対する精度は良いがテストデータに対する精度が低くなるということがしばしば起きます。このことを**過学習（overfitting）**もしくは**過剰学習**と呼び、モデル構築の段階において最も注意を要する検証事項です。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d2zV1N9XQ-xz"},"outputs":[],"source":["# データ分割（訓練データとテストデータ）のためのインポート\n","from sklearn.model_selection import train_test_split\n","\n","# 重回帰のモデル構築のためのインポート\n","from sklearn.linear_model import LinearRegression\n","\n"," # 目的変数にpriceを指定、説明変数にそれ以外を指定\n","X = auto.drop('price', axis=1)\n","y = auto['price']\n","\n","# 訓練データとテストデータに分ける\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n","\n","# 重回帰クラスの初期化と学習\n","model = LinearRegression()\n","model.fit(X_train,y_train)\n","\n","# 決定係数を表示\n","print('決定係数(train):{:.3f}'.format(model.score(X_train,y_train)))\n","print('決定係数(test):{:.3f}'.format(model.score(X_test,y_test)))\n","\n","# 回帰係数と切片を表示\n","print('\\n回帰係数\\n{}'.format(pd.Series(model.coef_, index=X.columns)))\n","print('切片: {:.3f}'.format(model.intercept_))"]},{"cell_type":"markdown","metadata":{"id":"YZO5YJW1Q-x0"},"source":["決定係数は`score`メソッドで取得できます。上記の結果では、`train`（訓練データ）で0.733、`test`（テストデータ）で0.737とわかります。訓練時スコアとテスト時のスコアが近いことから、このモデルは過学習に陥ってはいないと判断できます。\n","\n","### 8.2.4 モデル構築とモデル評価の流れのまとめ\n","\n","以上が重回帰によるモデル構築とモデル評価の流れです。以下で学ぶ決定木やSVMなども基本的に同じ流れで実行していきます。つまり、以下の流れがモデル構築とモデル評価の基本であることを押さえましょう。"]},{"cell_type":"markdown","metadata":{"id":"BAaj1v2_Q-x0"},"source":["- 各種モデル構築のためのクラスのインスタンス化：`model = LinearRegression()`\n","- データを説明変数と目的変数に分ける：$X$と$y$\n","- 訓練データとテストデータに分ける：`train_test_split(X, y, test_size=0.5, random_state=0)`\n","- 訓練データによるあてはめ（学習）：`model.fit(X_train, y_train)`\n","- モデルの汎化性能をテストデータで確かめる：`model.score(X_test, y_test)`"]},{"cell_type":"markdown","metadata":{"id":"e62hTlqZQ-x2"},"source":["ここでは、モデル構築の際、使用する説明変数として`horsepower`、`width`、`height`の3つを恣意的に選択しましたが、統計的に選択する方法もいくつかあります。具体的には、**変数増加法（前進的選択法）**、**変数減少法（後退的選択法）**、**ステップワイズ法**などで、選択するための規準も、RMSE（Root Mean Squared Error）、赤池情報量規準（AIC）、ベイズ情報量規準（BIC）などがあります。これらも絶対的にこの方法が有効というものではなく、モデルの汎化性能であったりビジネスドメイン知識なども考慮され選択されます。上記方法についての詳細は本書では割愛しますので、さらに学習を深めたい方は調べてみてください。"]},{"cell_type":"markdown","metadata":{"id":"ZIiJ-0sCQ-x2"},"source":[">**[やってみよう]**\n",">\n",">変数増加法、変数減少法、ステップワイズ法について調べてみましょう。"]},{"cell_type":"markdown","metadata":{"id":"mqrqQYj8Q-x3"},"source":["####  <練習問題 8-1>\n","本編で利用した自動車価格データを利用します。\n","このデータに対して、目的変数を`price`とし、説明変数に`length`と`engine-size`を使って重回帰のモデル構築をしてみましょう。このとき`train_test_split`を使って訓練データとテストデータが半分になるように分けてモデルを構築し、テストデータを使って、モデルのスコアを求めてください。`train_test_split`を実行する際には、`random_state`オプションを0に設定してください。"]},{"cell_type":"markdown","metadata":{"id":"NDm1JCSeQ-x3"},"source":["####  <練習問題 8-2>\n","<練習問題 8-1>のデータに対して、目的変数は同じ`price`で、上記とは別の説明変数を使って重回帰のモデル構築をしてみましょう。ただし、訓練データとテストデータが半分になるように分けて学習して、テストデータでスコアを求めてください。なお、訓練データとテストデータ分ける関数の`random_state`は0に設定して実施してください。モデルの結果がどのように変わったでしょうか。またその原因を考察してみましょう。"]},{"cell_type":"markdown","metadata":{"id":"2P6Ca1T8Q-x3"},"source":["## 8.3 ロジスティック回帰\n","キーワード：ロジスティック回帰、交差エントロピー誤差関数、オッズ比"]},{"cell_type":"markdown","metadata":{"id":"XzP8otyFQ-x3"},"source":["先に見てきたとおり、重回帰モデルは説明変数が複数ある回帰モデルで、目的変数は数値でした。このような変数を数値変数と言います。\n","\n","本節で学ぶ**ロジスティック回帰（logistic regression）**は目的変数が数値ではなく、たとえば、ある商品を買うか買わないか、ある会社が倒産するかしないかといった、カテゴリのデータを扱うアルゴリズムです。このようにカテゴリの形になっている変数をカテゴリ変数と言います。"]},{"cell_type":"markdown","metadata":{"id":"rnFVWrEFQ-x3"},"source":["データサンプルが、あるカテゴリに属するかどうかの確率を計算するタスクを分類（classification）といい、そのためのアルゴリズムの1つがロジスティック回帰です。\n","\n","回帰という名前がついていますが、分類を扱うアルゴリズムですので注意しましょう（また、2分類だけではなく3分類以上についても使えます）。目的変数が数値の時と違い、分類タスクでは以下の目的関数が最小になるように学習します。この目的関数を**交差エントロピー誤差関数（cross-entropy error function）**と言い、正解カテゴリを予測できる確率が高くなるほど値が小さくなります。"]},{"cell_type":"markdown","metadata":{"id":"_PYXVhpJQ-x3"},"source":["\\begin{eqnarray}\n","-\\sum^n_{i=1}[y_ilog(f(x_i))+(1-y_i)log(1-f(x_i))]\n","\\end{eqnarray}"]},{"cell_type":"markdown","metadata":{"id":"Ft5HiF8UQ-x3"},"source":["### 8.3.1 ロジスティック回帰の例\n","\n","それでは具体的にロジスティック回帰の実行例を見ていきましょう。ここでは、年齢や性別、職業などの個人に関するデータから、その人の収入が50K（5万ドル）を超えるかどうかを予測するためのモデルを構築してみましょう。元となるデータは、次のURLで取得できるものとします。\n","\n","http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n","\n","まずは次のようにデータを取得し、カラム名を設定します。データは32561行15例で構成されており、欠損値はありません。`head()`を使ってデータの先頭を見ると、`wrokclass`や`education`などのカテゴリ変数と`age`や`education_num`などの数値変数が混在したデータセットであるとわかります。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ClCq7uqWQ-x4"},"outputs":[],"source":["# データを取得\n","url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n","res = requests.get(url).content\n","\n","# 取得したデータをDataFrameオブジェクトとして読み込み\n","adult = pd.read_csv(io.StringIO(res.decode('utf-8')), header=None)\n","\n","# データの列にラベルを設定\n","adult.columns =['age','workclass','fnlwgt','education','education-num','marital-status',\n","                             'occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week',\n","                             'native-country','flg-50K']\n","\n","\n","# データの形式と欠損数を出力\n","print('データの形式:{}'.format(adult.shape))\n","print('欠損の数:{}'.format(adult.isnull().sum().sum()))\n","\n","# データの先頭5行を出力\n","adult.head()"]},{"cell_type":"markdown","metadata":{"id":"Z2aBzXzoQ-x4"},"source":["### 8.3.2 データの整理\n","\n","このデータセットにおいて、収入が50Kを超えるかどうかを示す目的変数は`flg-50K`です。データの値は「`<=50K`」と「`>50K`」で、このままでは扱いにくいので、0または1のフラグが入った変数に変換します。まずは、「`<=50K`」と「`>50K`」の行が、それぞれいくつあるかを確認してみます。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hOCvcIXvQ-x4"},"outputs":[],"source":["adult.groupby('flg-50K').size()"]},{"cell_type":"markdown","metadata":{"id":"3AIXgSiHQ-x4"},"source":["「<=50K」が24,720行、「>50K」が7,841行であることが分かります。"]},{"cell_type":"markdown","metadata":{"id":"VwPshFQSQ-x4"},"source":["次に、「fin_flg」というカラムを追加して、「`>50K`」である行には1、それ以外は0とフラグ立てをします。フラグ立てには`lambda`や`map`を使います。変換したら、念のため上の集計結果と同じであることをチェックします。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uowtvbjBQ-x4"},"outputs":[],"source":["# 「fin_flg」カラムを追加し、もし「flg-50K」カラムの値が「>50K」だったら1、そうでなければ0をセットする\n","adult['fin_flg'] = adult['flg-50K'].map(lambda x: 1 if x ==' >50K' else 0)\n","adult.groupby('fin_flg').size()"]},{"cell_type":"markdown","metadata":{"id":"H5Rk0DfJQ-x4"},"source":["「<=50K」と「>50K」の行数が、「0」と「1」の行数と一致したのでうまくいったことが分かります。"]},{"cell_type":"markdown","metadata":{"id":"BFbLjmoyQ-x5"},"source":["### 8.3.3 モデル構築と評価\n","いよいよロジスティク回帰のモデル構築です。説明変数として、数値変数の`age`、`fnlwgt`、`education-num`、`capital-gain`、`capital-loss`を使うことにします。目的変数は、先ほど「1」と「0」のフラグを立てた`fin_flg`です。\n","\n","ロジスティック回帰のモデル構築には`LogisticRegression`クラスを使います。訓練データとテストデータに分けたり、`score`メソッドで評価したりする方法は、重回帰のときと同じです。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v2ztowE4Q-x5"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","\n","# 説明変数と目的変数の設定\n","X = adult[['age','fnlwgt','education-num','capital-gain','capital-loss']]\n","y = adult['fin_flg']\n","\n","# 訓練データとテストデータに分ける\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n","\n","# ロジスティック回帰クラスの初期化と学習\n","model = LogisticRegression()\n","model.fit(X_train,y_train)\n","\n","print('正解率(train):{:.3f}'.format(model.score(X_train, y_train)))\n","print('正解率(test):{:.3f}'.format(model.score(X_test, y_test)))"]},{"cell_type":"markdown","metadata":{"id":"OS-_EFznQ-x5"},"source":["上記の結果から、訓練データとテストデータともに約79%の正解率であり、過学習は起きていないと判断できます。"]},{"cell_type":"markdown","metadata":{"id":"EtLal4S6Q-x5"},"source":["学習済みモデルの各変数（age、fnlwgt、education-num、capital-gain、capital-loss）の係数を、`coef_`属性を取得することで確認してみます。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zijYPo5uQ-x5"},"outputs":[],"source":["model.coef_"]},{"cell_type":"markdown","metadata":{"id":"C2-VZz3PQ-x5"},"source":["また、それぞれのオッズ比は以下のように算出できます。オッズ比とは、それぞれの係数が1増加したとき、正解率にどの程度影響があるかを示す指標です。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ArqFKRcTQ-x6"},"outputs":[],"source":["np.exp(model.coef_)"]},{"cell_type":"markdown","metadata":{"id":"t7J_SBjPQ-x6"},"source":["### 8.3.4 スケーリングによる予測精度の向上\n","\n","ここで予測精度を上げるためのアプローチの1つであるスケーリングについて紹介します。このモデルでは`age`、`fnlwgt`、`education-num`、`capital-gain`、`capital-los`の5つの説明変数を使っていますが、それぞれの単位や大きさは異なっています。このままだとモデルの学習が値の大きな変数に引っ張られ値の小さな変数の影響が小さくなる懸念があります。\n","\n","そこでそうならないようにするため、説明変数の標準化を実施します。標準化とはスケーリングの一種で、データの各値から変数列の平均を引き、標準偏差で割ります。こうすることで変数間の単位が消え数値の大小と意味するところが合致します（値が0ならそれは平均値、1なら1標準偏差だけ平均値より大きい値とわかります）。データを標準化するには`StandardScaler`クラスを使います。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FDu5CQo0Q-x6"},"outputs":[],"source":["# 標準化のためのクラスをインポート\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","\n","# Xとyを設定\n","X = adult[['age','fnlwgt','education-num','capital-gain','capital-loss']]\n","y = adult['fin_flg']\n","\n","# 訓練データとテストデータに分ける\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n","\n","# 標準化処理\n","sc = StandardScaler()\n","sc.fit(X_train)\n","X_train_std = sc.transform(X_train)\n","X_test_std = sc.transform(X_test)\n","\n","# ロジスティック回帰クラスの初期化と学習\n","model = LogisticRegression()\n","model.fit(X_train_std,y_train)\n","\n","# 正解率の表示\n","print('正解率(train):{:.3f}'.format(model.score(X_train_std, y_train)))\n","print('正解率(test):{:.3f}'.format(model.score(X_test_std, y_test)))"]},{"cell_type":"markdown","metadata":{"id":"SFo3vDQjQ-x6"},"source":["上記の結果を見るとわかるように、標準化しない場合に比べて正解率が上昇しています。このように説明変数の尺度を揃えることで、機械学習のアルゴリズムをよりうまく動作させられます。標準化処理で留意しておきたいポイントは、訓練データの平均値と標準偏差を使用している点です。テスト用データは将来手に入るであろう未知データという位置づけですから、そのデータを使ってモデルを標準化することはできません。"]},{"cell_type":"markdown","metadata":{"id":"Ur4laEW4Q-x6"},"source":["####  <練習問題 8-3>\n","`sklearn.datasets`モジュールの`load_breast_cancer`関数を使って乳がんデータを読み込んで、目的変数を`cancer.target`として、`cancer.data`を説明変数にロジスティック回帰で予測モデルを構築してください。この時、訓練データとテストデータに分ける`train_test_split（random_state=0）`を使って、テストデータにおけるスコアを求めてください。"]},{"cell_type":"markdown","metadata":{"id":"cyWaooSDQ-x6"},"source":["####  <練習問題 8-4>\n","<練習問題 8-3>と同じ設定で、同じデータに対して、特徴量を標準化してモデル構築してみてください。その上で、上記の結果と比較してください。"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[{"file_id":"1hFsFJ2E0-KSud0MufGjk2bva3uaoBLr4","timestamp":1750829936740},{"file_id":"1uXAQDGxCF5bQXdKzp_-iF8BgtIY2eUJy","timestamp":1750824369180}],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}